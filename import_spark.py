# -*- coding: utf-8 -*-
"""Import Spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B3cwmStRZcoTPwYDnUqX7CJ6G-cYzuct
"""

# Instalar SDK Java 8
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# Descargar Spark 3.4.3
!wget -q -P . https://archive.apache.org/dist/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz

# Descomprimir el archivo descargado de Spark
!tar xf spark-3.4.3-bin-hadoop3.tgz

# Establecemos las variables de entorno
import os

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.3-bin-hadoop3"

# Instalamos la librería findspark para facilitar el uso de Apache Spark en un entorno interactivo como Jupyter notebooks o shells interactivos de Python.
!pip install -q findspark

# Instalar pyspark
!pip install -q pyspark

import findspark

findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

spark

"""#Primer trabajo"""

data = [("Spark", "Python")] * 10
df = spark.createDataFrame(data, ["Columna 1 Big Data", "Columna 2 Big Data"])

#Mostrar
df.show()

"""#Segundo trabajo"""

#Crear
data = [("Spark", "Explora Python es un lenguaje versátil")] * 10
df = spark.createDataFrame(data, ["Columna 1 Big Data", "Columna 2 Big Data"])

#Mostrar
df.show(truncate=False)