{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IVuAAtjkFIGu"
      },
      "outputs": [],
      "source": [
        "#Instalar SDK JAVA 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar Spark 3.4.3\n",
        "!wget -q -P . https://archive.apache.org/dist/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "R2cNvMbRFneF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descomprimir el archivo descargado de Spark\n",
        "!tar xf spark-3.4.3-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "HopRY_NBGcIO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecemos las variables de entorno\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\""
      ],
      "metadata": {
        "id": "a2WQy9umGeoD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos la librería findspark para facilitar el uso de Apache Spark en un entorno interactivo como Jupyter notebooks o shells interactivos de Python.\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "u5JdmFVyGinD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instalar pyspark\n",
        "!pip install -q pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtrFl3MeGmZU",
        "outputId": "853444f5-1aed-414c-abee-0d0f40ec5683"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "7qPVeMERGuWW",
        "outputId": "3f101356-d1e7-4124-c600-fd1de5d5649e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ad1a9f4cac0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4c1884fc8325:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Esta Celda la ejecutan en otro Google Colab para que puedan verificar el cambio de nombre de la sesión de Spark.\n",
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName('CERTUS Pyspark - Antonella').getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "Rq4ZHfLfGyD1",
        "outputId": "a5726357-6ca5-4298-f7b8-cb738021efcf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ad1a9f4cac0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4c1884fc8325:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Probando la sesión de Spark\n",
        "\n",
        "df = spark.createDataFrame(curse, ['Columna 1 Big Data' , 'Columna 2 Big Data'])\n",
        "curse = [('Spark','Python') for i in range(10)]\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVpnO4OMT-Ee",
        "outputId": "6ef4fbe7-6dba-41ed-8513-d447384d8dc9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+------------------+\n",
            "|Columna 1 Big Data|Columna 2 Big Data|\n",
            "+------------------+------------------+\n",
            "|             Spark|            Python|\n",
            "|             Spark|            Python|\n",
            "|             Spark|            Python|\n",
            "|             Spark|            Python|\n",
            "|             Spark|            Python|\n",
            "|             Spark|            Python|\n",
            "|             Spark|            Python|\n",
            "|             Spark|            Python|\n",
            "|             Spark|            Python|\n",
            "|             Spark|            Python|\n",
            "+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame( curse , ['Columna 1 Big Data' , 'Columna 2 Big Data'])\n",
        "curse = [('Spark','Explora Python es un lenguaje versátil') for i in range(10)]\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwjCHOjKlNTX",
        "outputId": "9e0e2680-9419-4d58-846c-dedee96f5684"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+--------------------------------------+\n",
            "|Columna 1 Big Data|Columna 2 Big Data                    |\n",
            "+------------------+--------------------------------------+\n",
            "|Spark             |Explora Python es un lenguaje versátil|\n",
            "|Spark             |Explora Python es un lenguaje versátil|\n",
            "|Spark             |Explora Python es un lenguaje versátil|\n",
            "|Spark             |Explora Python es un lenguaje versátil|\n",
            "|Spark             |Explora Python es un lenguaje versátil|\n",
            "|Spark             |Explora Python es un lenguaje versátil|\n",
            "|Spark             |Explora Python es un lenguaje versátil|\n",
            "|Spark             |Explora Python es un lenguaje versátil|\n",
            "|Spark             |Explora Python es un lenguaje versátil|\n",
            "|Spark             |Explora Python es un lenguaje versátil|\n",
            "+------------------+--------------------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}