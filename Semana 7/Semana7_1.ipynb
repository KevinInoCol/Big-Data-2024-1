{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kzaaVmefEu4z"
      },
      "outputs": [],
      "source": [
        "# Instalar SDK Java 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar Spark 3.4.3\n",
        "!wget -q -P . https://archive.apache.org/dist/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "BGNvN4rIGcCm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descomprimir el archivo descargado de Spark\n",
        "!tar xf spark-3.4.3-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "gccScdSiGfEI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecemos las variables de entorno\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\""
      ],
      "metadata": {
        "id": "lom2xfTRGjac"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos la librería findspark para facilitar el uso de Apache Spark en un entorno interactivo como Jupyter notebooks o shells interactivos de Python.\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "50rt82K8Gmw0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar pyspark\n",
        "!pip install -q pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf0KU8SLGp_n",
        "outputId": "1a3d4f9c-ffbb-4f82-b634-6dad9d40df31"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark #Jupyter\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "ZMfB4_TpHWJx",
        "outputId": "e3e89a2c-47fa-4c8c-af21-9d148923e48a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b171c09ee60>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8944565b3700:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Esta Celda la ejecutan en otro Google Colab para que puedan verificar el cambio de nombre de la sesión de Spark.\n",
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName('CERTUS Pyspark - Kevin Inofuente').getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "O5jAYNyYHx7j",
        "outputId": "481789d7-00c5-4e8b-8b0f-063317e5fad6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b171c09ee60>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8944565b3700:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Probando la sesión de Spark\n",
        "#dataframe = Objeto.funcionHeredada\n",
        "df = spark.createDataFrame([                               for   in          ])"
      ],
      "metadata": {
        "id": "gKe5A2sHHypy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "# Crear una sesión Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Crear DataFrame con valores específicos\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Definir el esquema del DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"columna 1 Big Data\", StringType(), True),\n",
        "    StructField(\"columna 2 Big Data\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Crear una lista de datos utilizando una lista de comprensión\n",
        "datos = [(\"Python\", \"Spark\") for _ in range(10)]\n",
        "\n",
        "# Crear el DataFrame a partir de la lista de tuplas y el esquema\n",
        "df = spark.createDataFrame(datos, schema)"
      ],
      "metadata": {
        "id": "XLHewNdeUHSu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM_rtDFCUItK",
        "outputId": "a172ddaf-7d50-4319-bb1f-ffe391566f94"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+------------------+\n",
            "|columna 1 Big Data|columna 2 Big Data|\n",
            "+------------------+------------------+\n",
            "|            Python|             Spark|\n",
            "|            Python|             Spark|\n",
            "|            Python|             Spark|\n",
            "|            Python|             Spark|\n",
            "|            Python|             Spark|\n",
            "|            Python|             Spark|\n",
            "|            Python|             Spark|\n",
            "|            Python|             Spark|\n",
            "|            Python|             Spark|\n",
            "|            Python|             Spark|\n",
            "+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "foJgbJAVUlkD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "# Crear una sesión Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Crear DataFrame con valores específicos\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Definir el esquema del DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"columna 1 Big Data\", StringType(), True),\n",
        "    StructField(\"columna 2 Big Data\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Crear una lista de datos utilizando una lista de comprensión\n",
        "datos = [(\"Spark\", \"Explora Python es un lenguaje versatil\") for _ in range(10)]\n",
        "\n",
        "# Crear el DataFrame a partir de la lista de tuplas y el esquema\n",
        "df = spark.createDataFrame(datos, schema)"
      ],
      "metadata": {
        "id": "bGnfOLV3WKCZ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtHbcev6WmrQ",
        "outputId": "2ef28cef-fa80-4ac8-ad45-ce1d39468b83"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+--------------------+\n",
            "|columna 1 Big Data|  columna 2 Big Data|\n",
            "+------------------+--------------------+\n",
            "|             Spark|Explora Python es...|\n",
            "|             Spark|Explora Python es...|\n",
            "|             Spark|Explora Python es...|\n",
            "|             Spark|Explora Python es...|\n",
            "|             Spark|Explora Python es...|\n",
            "|             Spark|Explora Python es...|\n",
            "|             Spark|Explora Python es...|\n",
            "|             Spark|Explora Python es...|\n",
            "|             Spark|Explora Python es...|\n",
            "|             Spark|Explora Python es...|\n",
            "+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "mjGo1q6UWoLN"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}