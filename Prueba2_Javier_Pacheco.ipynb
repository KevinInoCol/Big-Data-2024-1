{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PuX2tXjvkYYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d8dddcb-c8a8-4a34-9afc-c2838ea69b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=fc506f8516711d22557ccde3b79096c20aef2c4cd22fa273b649a41334c2a551\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "#Instalamos pyspark y py4j\n",
        "!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "Spark = SparkSession.builder.appName(\"jdpr\").getOrCreate()\n",
        "SparkContext = Spark.sparkContext"
      ],
      "metadata": {
        "id": "eRte-hLfEfFk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Cree un RDD llamado lenguajes que contenga los siguientes lenguajes de programación: Python,\n",
        "#R, C, Scala, Rugby y SQL.\n",
        "\n",
        "rdd_lenguajes = SparkContext.parallelize(['Python', 'C', 'Scala','Rugby', 'SQL'])\n"
      ],
      "metadata": {
        "id": "bVu3iR1YFBKs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#       C) Cree un nuevo RDD que solo contenga aquellos lenguajes de\n",
        "#         programación que comiencen con la letra R.\n",
        "\n",
        "rdd_filtrado_con_R = rdd_lenguajes.filter(lambda x: x.startswith('R'))\n",
        "rdd_filtrado_con_R.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owOf_MKoFFvO",
        "outputId": "0ab3db19-3dc5-4320-9b5d-e57f01037174"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Rugby']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Cree un RDD llamado pares que contenga los números pares existentes en el intervalo [20;30].\n",
        "rdd_pares = SparkContext.parallelize(range(20, 30, 2))"
      ],
      "metadata": {
        "id": "BrEOl8ryF54h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#       B) Obtenga una lista compuesta por los números pares en el intervalo\n",
        "#         [20;30] y sus respectivas raíces cuadradas. Un ejemplo del resultado\n",
        "#         deseado para el intervalo [50;60] sería la lista [50, 7.0710678118654755, 52, 7.211102550927978,\n",
        "#         54, 7.3484692283495345, 56, 7.483314773547883, 58, 7.615773105863909, 60, 7.745966692414834].\n",
        "\n",
        "rdd_lista_raices = rdd_pares.flatMap(lambda x: [(x, x**0.5)])\n",
        "rdd_rintervalos = rdd_lista_raices.filter(lambda x: x[0] % 2 == 0)\n",
        "rdd_rintervalos.collect()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkY8ANuNHbIk",
        "outputId": "24a0f993-8695-4ccf-a697-3edd1ecceaf6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(20, 4.47213595499958),\n",
              " (22, 4.69041575982343),\n",
              " (24, 4.898979485566356),\n",
              " (26, 5.0990195135927845),\n",
              " (28, 5.291502622129181)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#       C) Eleve el número de particiones del RDD sqrt a 20.\n",
        "rdd_rintervalos_elevados = rdd_rintervalos.repartition(20)\n",
        "rdd_rintervalos_elevados.collect()"
      ],
      "metadata": {
        "id": "rDDriinOIM7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074eedc2-d8e4-4add-ec73-207cf2850917"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(20, 4.47213595499958),\n",
              " (22, 4.69041575982343),\n",
              " (24, 4.898979485566356),\n",
              " (26, 5.0990195135927845),\n",
              " (28, 5.291502622129181)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#       D) Si tuviera que disminuir el número de particiones luego de haberlo\n",
        "#          establecido en 20, ¿qué función utilizaría para hacer más eficiente su código?\n",
        "#   Para disminuir el número de particiones luego de haberlo establecido en 20, puedo utilizar la función coalesce(). coalesce()\n",
        "#   Ya que es una función de optimización que fusiona particiones contiguas para reducir el número total de particiones.\n",
        "# EJEMPLO\n",
        "rdd_pares_reducido = rdd_pares.coalesce(10)"
      ],
      "metadata": {
        "id": "kcVSHRWYoLuN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Cree un RDD del tipo clave valor a partir de la lectura del txt de transacciones.\n",
        "#Tenga en cuenta que deberá procesar el RDD leído para obtener el resultado solicitado. Supongamos\n",
        "#que el RDD resultante de tipo clave valor refleja las transacciones realizadas por número de\n",
        "#cuentas. Obtenga el monto total por cada cuenta (Suma).\n",
        "#Tip: Cree su propia función para procesar el RDD leído.\n",
        "\n",
        "rdd_transacciones = SparkContext.parallelize([\n",
        "    (1001, 52.3),\n",
        "    (1005, 20.8),\n",
        "    (1001, 10.1),\n",
        "    (1004, 52.7),\n",
        "    (1005, 20.7),\n",
        "    (1002, 85.3),\n",
        "    (1004, 20.9)\n",
        "])\n",
        "rdd_transacciones.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFEKJWvjrD2w",
        "outputId": "abb27499-7801-4338-eff8-c3e46632c468"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1001, 52.3),\n",
              " (1005, 20.8),\n",
              " (1001, 10.1),\n",
              " (1004, 52.7),\n",
              " (1005, 20.7),\n",
              " (1002, 85.3),\n",
              " (1004, 20.9)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_reducido_por_clave = rdd_transacciones.reduceByKey(lambda x,y: x + y)\n",
        "rdd_reducido_por_clave.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFiI5gxzuEr0",
        "outputId": "1f87cdad-3a34-4e5b-f48c-64ecb7262237"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1004, 73.6), (1002, 85.3), (1001, 62.4), (1005, 41.5)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}